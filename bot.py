import os
import asyncio
from typing import Dict, Optional

from telegram import Update
from telegram.ext import (
    Application, CommandHandler, MessageHandler, ContextTypes, filters
)

# --- OpenAI (GPT)
from openai import AsyncOpenAI

# --- xAI (Grok)
import httpx

# --- Google Gemini
import google.generativeai as genai


"""
HÆ¯á»šNG DáºªN TRIá»‚N KHAI (tÃ³m táº¯t):
1) Táº¡o bot vÃ  láº¥y TELEGRAM_BOT_TOKEN tá»« BotFather.
2) Thiáº¿t láº­p biáº¿n mÃ´i trÆ°á»ng:
   - TELEGRAM_BOT_TOKEN
   - OPENAI_API_KEY
   - XAI_API_KEY
   - GEMINI_API_KEY
   (trÃªn Heroku: Settings > Config Vars)
3) Deploy lÃªn Heroku (hoáº·c ná»n táº£ng khÃ¡c) vÃ  báº­t dyno 'worker'.
"""

# =========================
# Cáº¥u hÃ¬nh & Khá»Ÿi táº¡o SDKs
# =========================
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
XAI_API_KEY = os.getenv("XAI_API_KEY", "")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")

# Model máº·c Ä‘á»‹nh (cÃ³ thá»ƒ Ä‘á»•i báº±ng biáº¿n mÃ´i trÆ°á»ng)
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
XAI_MODEL = os.getenv("XAI_MODEL", "grok-2-latest")
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")

if not TELEGRAM_BOT_TOKEN:
    raise RuntimeError("Thiáº¿u TELEGRAM_BOT_TOKEN")

# OpenAI (GPT)
openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

# xAI (Grok) dÃ¹ng httpx
httpx_client = httpx.AsyncClient(timeout=60.0) if XAI_API_KEY else None

# Gemini
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    gemini_model = genai.GenerativeModel(GEMINI_MODEL)
else:
    gemini_model = None

# LÆ°u cháº¿ Ä‘á»™ chat theo chat_id: {'mode': 'gpt'|'grok'|'gemini'}
SESSION_MODE: Dict[int, str] = {}


# ============
# AI helpers
# ============
async def ask_gpt(prompt: str, user_id: int) -> str:
    if not openai_client:
        return "âš ï¸ ChÆ°a cáº¥u hÃ¬nh OPENAI_API_KEY."
    try:
        resp = await openai_client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "Báº¡n lÃ  má»™t trá»£ lÃ½ thÃ¢n thiá»‡n, tráº£ lá»i ngáº¯n gá»n vÃ  há»¯u Ã­ch."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.7,
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        return f"âŒ Lá»—i GPT: {e}"


async def ask_grok(prompt: str, user_id: int) -> str:
    if not httpx_client:
        return "âš ï¸ ChÆ°a cáº¥u hÃ¬nh XAI_API_KEY."
    try:
        url = "https://api.x.ai/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {XAI_API_KEY}",
            "Content-Type": "application/json",
        }
        payload = {
            "model": XAI_MODEL,
            "messages": [
                {"role": "system", "content": "Báº¡n lÃ  má»™t trá»£ lÃ½ ngáº¯n gá»n, chÃ­nh xÃ¡c."},
                {"role": "user", "content": prompt},
            ],
            "temperature": 0.7,
        }
        r = await httpx_client.post(url, headers=headers, json=payload)
        r.raise_for_status()
        data = r.json()
        text = data.get("choices", [{}])[0].get("message", {}).get("content", "")
        return (text or "").strip() or "âš ï¸ Grok khÃ´ng tráº£ vá» ná»™i dung."
    except Exception as e:
        return f"âŒ Lá»—i Grok: {e}"


async def ask_gemini(prompt: str, user_id: int) -> str:
    if not gemini_model:
        return "âš ï¸ ChÆ°a cáº¥u hÃ¬nh GEMINI_API_KEY."
    try:
        # ThÆ° viá»‡n Gemini hiá»‡n Ä‘á»“ng bá»™ -> cháº¡y trong thread Ä‘á»ƒ khÃ´ng cháº·n loop
        def _run():
            resp = gemini_model.generate_content(prompt)
            return (resp.text or "").strip()
        text = await asyncio.to_thread(_run)
        return text or "âš ï¸ Gemini khÃ´ng tráº£ vá» ná»™i dung."
    except Exception as e:
        return f"âŒ Lá»—i Gemini: {e}"


# =================
# Telegram Handlers
# =================
WELCOME = (
    "ğŸ‘‹ ChÃ o báº¡n Ä‘áº¿n vá»›i bot Telegram cá»§a nhÃ  phÃ¡t triá»ƒn **TÃ´ Minh Äiá»m**.\n"
    "Báº¡n cÃ³ thá»ƒ trÃ² chuyá»‡n vá»›i cÃ¡c AI sau. GÃµ /help Ä‘á»ƒ xem lá»‡nh.\n\n"
    "â€¢ /gpt â€“ Chat vá»›i GPT (OpenAI)\n"
    "â€¢ /grok â€“ Chat vá»›i Grok (xAI)\n"
    "â€¢ /gemini â€“ Chat vá»›i Gemini (Google)\n"
)

HELP_TEXT = (
    "ğŸ†˜ HÆ°á»›ng dáº«n lá»‡nh:\n"
    "â€¢ /gpt <cÃ¢u há»i> â€” há»i nhanh vá»›i GPT\n"
    "â€¢ /grok <cÃ¢u há»i> â€” há»i nhanh vá»›i Grok\n"
    "â€¢ /gemini <cÃ¢u há»i> â€” há»i nhanh vá»›i Gemini\n\n"
    "Máº¹o: Báº¡n cÃ³ thá»ƒ vÃ o cháº¿ Ä‘á»™ há»™i thoáº¡i liÃªn tá»¥c vá»›i má»™t AI báº±ng cÃ¡ch gÃµ má»—i lá»‡nh *khÃ´ng kÃ¨m cÃ¢u há»i*.\n"
    "VÃ­ dá»¥: gÃµ `/gpt` rá»“i gá»­i tin nháº¯n bÃ¬nh thÆ°á»ng, bot sáº½ hiá»ƒu lÃ  nháº¯n cho GPT cho Ä‘áº¿n khi Ä‘á»•i lá»‡nh khÃ¡c."
)


async def start_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_markdown(WELCOME)


async def help_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_markdown(HELP_TEXT)


def extract_prompt(args: list[str]) -> str:
    return " ".join(args).strip() if args else ""


async def set_mode_and_optionally_ask(
    update: Update,
    context: ContextTypes.DEFAULT_TYPE,
    mode: str
):
    chat_id = update.effective_chat.id
    SESSION_MODE[chat_id] = mode

    prompt = extract_prompt(context.args)
    if not prompt:
        await update.message.reply_text(
            f"âœ… ÄÃ£ chuyá»ƒn sang cháº¿ Ä‘á»™ {mode.upper()}. HÃ£y gá»­i tin nháº¯n Ä‘á»ƒ trÃ² chuyá»‡n!"
        )
        return

    await update.message.chat.send_action(action="typing")
    if mode == "gpt":
        answer = await ask_gpt(prompt, chat_id)
    elif mode == "grok":
        answer = await ask_grok(prompt, chat_id)
    else:
        answer = await ask_gemini(prompt, chat_id)

    await update.message.reply_text(answer)


async def gpt_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await set_mode_and_optionally_ask(update, context, "gpt")


async def grok_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await set_mode_and_optionally_ask(update, context, "grok")


async def gemini_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await set_mode_and_optionally_ask(update, context, "gemini")


async def text_router(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not update.message or not update.message.text:
        return
    chat_id = update.effective_chat.id
    text = update.message.text.strip()

    mode: Optional[str] = SESSION_MODE.get(chat_id)
    if not mode:
        # ChÆ°a chá»n AI -> gá»£i Ã½ lá»‡nh
        await update.message.reply_text(
            "Báº¡n muá»‘n chat vá»›i AI nÃ o?\nDÃ¹ng /gpt, /grok, hoáº·c /gemini (cÃ³ thá»ƒ kÃ¨m cÃ¢u há»i)."
        )
        return

    await update.message.chat.send_action(action="typing")
    if mode == "gpt":
        answer = await ask_gpt(text, chat_id)
    elif mode == "grok":
        answer = await ask_grok(text, chat_id)
    else:
        answer = await ask_gemini(text, chat_id)

    await update.message.reply_text(answer)


async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE) -> None:
    # Log nháº¹ nhÃ ng, tráº£ lá»i ngÆ°á»i dÃ¹ng
    try:
        if isinstance(update, Update) and update.effective_message:
            await update.effective_message.reply_text("âŒ ÄÃ£ xáº£y ra lá»—i khÃ´ng mong muá»‘n. Vui lÃ²ng thá»­ láº¡i.")
    finally:
        print(f"Exception: {context.error}")


def main():
    app = Application.builder().token(TELEGRAM_BOT_TOKEN).build()

    app.add_handler(CommandHandler("start", start_cmd))
    app.add_handler(CommandHandler("help", help_cmd))

    app.add_handler(CommandHandler("gpt", gpt_cmd))
    app.add_handler(CommandHandler("grok", grok_cmd))
    app.add_handler(CommandHandler("gemini", gemini_cmd))

    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, text_router))

    app.add_error_handler(error_handler)

    # DÃ¹ng long-polling (phÃ¹ há»£p dyno worker)
    print("Bot is runningâ€¦")
    app.run_polling(close_loop=False)


if __name__ == "__main__":
    try:
        main()
    finally:
        # ÄÃ³ng httpx client náº¿u cÃ³
        if httpx_client:
            asyncio.get_event_loop().run_until_complete(httpx_client.aclose())
